<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>multilevel modeling | DYLAN WIWAD, Ph.D</title>
    <link>https://dwiwad.github.io/tag/multilevel-modeling/</link>
      <atom:link href="https://dwiwad.github.io/tag/multilevel-modeling/index.xml" rel="self" type="application/rss+xml" />
    <description>multilevel modeling</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 18 Mar 2022 00:00:00 -0500</lastBuildDate>
    <image>
      <url>https://dwiwad.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>multilevel modeling</title>
      <link>https://dwiwad.github.io/tag/multilevel-modeling/</link>
    </image>
    
    <item>
      <title>Why are Tinder App Store Ratings Declining?</title>
      <link>https://dwiwad.github.io/project/tinder_ratings/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 -0500</pubDate>
      <guid>https://dwiwad.github.io/project/tinder_ratings/</guid>
      <description>&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tinder App Store ratings declined 42% Between 2013 and 2022 in two waves.&lt;/li&gt;
&lt;li&gt;I analyzed 7.8 Million words left across 525,000 reviews to find out why.&lt;/li&gt;
&lt;li&gt;First decline: related to to higher user complaints about Bugs and Spam (e.g., fake users).&lt;/li&gt;
&lt;li&gt;Second decline: related only to higher complaints of Spam as the app became more stable&lt;/li&gt;
&lt;li&gt;Tinder should enact more stringent controls on fake users to increase user engagement, sentiment, and retention&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;executive-summary&#34;&gt;Executive Summary:&lt;/h2&gt;
&lt;p&gt;Between 2013 and 2022 Tinder app ratings on the Google Play store have fallen 42% From a peak of 4.05 (out of 5) in October 2014 to 2.35 (out of 5) as of February 2022. This occurred over two steady declines; ratings declined between July 16, 2013 and September 24, 2015, recovered partially, and then declined again between October 14, 2018 and January 13, 2021. To explore why, I analyzed 525,294 written reviews (approximately 7.8 million words) using Natural Language Processing techniques (e.g., term-document frequencies, sentiment analysis, and word embeddings) and ordinary piecewise and linear regression. I find that the eight-year decline in Tinder ratings coincides with 50% increase in negative review content (compared with only a 41% decrease in positive review content). More specifically, complaints of both ‘fake’ users (e.g., bots, scammers, etc) and bugs (e.g., crashing, freezing, etc) predict Tinder’s initial ratings decline. However, only complaints surrounding fake users—but not bugs—predicts Tinder’s second more recent decline in ratings. These data suggest that, while they have sufficiently stabilized the app, the organization still needs to enact more stringent controls on ‘fake’ users in order to increase engagement, usage, sentiment, and retention. All R code for this project can be found 
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;I was interested in exploring some data regarding how engagement with a mobile application changes over time and I found 
&lt;a href=&#34;https://www.kaggle.com/sidharthkriplani/datingappreviews&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; dataset containing 681,993 dating app reviews for Tinder, Bumble, and Hinge spanning 2013 to 2022. It’s a very bare-bones dataset containing only the user’s name, their plain-text review, their numeric star rating of the app (coded numerically as 1 to 5 corresponding to the number of stars), the number of “thumbs up” endorsements that the review got, the date of the review, and the app name. See the end of this post for detailed methodological detail.&lt;/p&gt;
&lt;p&gt;I was immediately curious about how ratings of the apps, in particular Tinder, have changed over time and what elements from the natural language data predict this change. As you can see below, the vast majority—77%—of the ratings are for Tinder. Further, Tinder had much faster growth settling around 60,000 – 70,000 reviews per year, compared with less than 25,000 per year for Hinge and Bumble.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1.reviews_per_app.png&#34; alt=&#34;Figure 1&#34;&gt;
&lt;img src=&#34;2.count_by_app_year.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Given this, I decided to focus solely on exploring the trajectory of the 526,615 Tinder ratings. I explored two key questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How are Tinder app ratings changing over time?&lt;/li&gt;
&lt;li&gt;If they are changing, why is that happening?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;how-are-tinder-app-ratings-changing-over-time&#34;&gt;How are Tinder App Ratings Changing Over Time?&lt;/h2&gt;
&lt;p&gt;Since its peak of 4.05 in October 2014 Tinder ratings have been steadily declining to its current position of 2.35 as of February 2022. This is 42% drop in average rating over the app’s 8-year life span.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3.tinder_rating_time_ci.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;Additionally, we see that this decline came in two waves. Ratings declined until late 2016, recovered slightly, and then declined sharply over 2019 and 2020, finally settling to its current low point.&lt;/p&gt;
&lt;p&gt;I used a simple linear regression method to quantify exactly how much ratings have declined over the last eight years. The standardized effect size from this regression was ß = -.12. While this may not seem like a ‘huge’ effect, it is worth noting that this is equivalent to the impact of being deployed to Vietnam on the development of PTSD, or the effect of an oral antihistamine on reduced sneezing.&lt;/p&gt;
&lt;p&gt;Clearly ratings having been declining over time in two predictable stages. In order to more concretely define these declines, I used a piecewise linear regression. This analysis extracts inflection points (“breakpoints”) at which the linear trend changes. Given my subjective assessment of four “phases” to the trend—initial decline, recovery, secondary decline, final low—I specified three breakpoints. Table 1 includes the model-defined start and end points for each phase of the ratings trend (See methods for full analysis).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Table_1.png&#34; alt=&#34;Table 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we turn to the actual text reviews—do they provide clues as to why these declines occurred?&lt;/p&gt;
&lt;h2 id=&#34;why-are-ratings-declining-over-time&#34;&gt;Why are Ratings Declining Over Time?&lt;/h2&gt;
&lt;p&gt;There are many ways to test this question. For the sake of brevity, I will focus on two elements of the reviews: emotional language and content analysis (e.g., what themes are present in the reviews). First, I am going to explore whether reviews are becoming (a) less positive, (b) more negative, or (c) both. Secondly, I will dig into the content of the negative words to explore any potential themes.&lt;/p&gt;
&lt;p&gt;I first pre-processed the raw text reviews to put it in a more usable format. Initially, the text contained 7,864,944 individual words (&lt;em&gt;M&lt;sub&gt;rev&lt;/sub&gt;&lt;/em&gt; = 14.93, &lt;em&gt;Med&lt;sub&gt;rev&lt;/sub&gt;&lt;/em&gt; = 7, &lt;em&gt;SD&lt;/em&gt; = 19.47 words) spread across 526,615 reviews. However, many reviews were either not in English or contained non-text elements (e.g., emojis). Additionally, it is important to remove stopwords (e.g., the, at, for)—words that are extremely frequent but are not meaningful. Following the removal of all stopwords and non-English text, I was left with a corpus of 4,383,647 words (&lt;em&gt;M&lt;sub&gt;rev&lt;/sub&gt;&lt;/em&gt; = 8.32, &lt;em&gt;Med&lt;sub&gt;rev&lt;/sub&gt;&lt;/em&gt; = 4, &lt;em&gt;SD&lt;/em&gt; = 10.25 words) across 520,929 reviews.&lt;/p&gt;
&lt;h3 id=&#34;emotion-content&#34;&gt;Emotion Content&lt;/h3&gt;
&lt;p&gt;First, for each review I simply computed the number of both positive (e.g., great, nice, fun) and negative (e.g., bad, stupid, boring). Following this, I computed the term-review frequency by computing the proportion of words in each review that were positive and negative. For example, if a review contained 10 words total and 4 were negative and 2 were positive the term-review frequencies are 0.40 and 0.20 for positive and negative words, respectively.&lt;/p&gt;
&lt;p&gt;As can be seen below, we see that over time reviews became both less positive (&lt;em&gt;ß&lt;/em&gt; = -.017, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.020, -.014]) and more negative (&lt;em&gt;ß&lt;/em&gt; = .052, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.049, .055]). Crucially, the increase in negative content outpaced the decrease in positive content. Specifically, while there was a 41% decrease in positive content, there was 50% increase in negative content.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;4.freq_emote_time.png&#34; alt=&#34;Figure 4&#34;&gt;&lt;/p&gt;
&lt;p&gt;Given that negative content is outpacing positive content, this is where I focused the final piece of my search—what exactly is it that users are complaining about? Is it something that could be addressed by the developers?&lt;/p&gt;
&lt;p&gt;In order to do this, I calculated how frequently each negative word occurs in the entire body of 308,729 unique words irrespective of when the review was left. I first computed the frequency of every individual word in the review text and then performed a sentiment analysis to classify each word as positive or negative.&lt;/p&gt;
&lt;p&gt;The most frequent negative word, occurring 18,353 times (5.94% of all negative words) is the word ‘fake.’ Another interesting element here is that a handful of these top 20 words pertain to bugs in the software—crashes, issues, bugs, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;5.top_20_neg.png&#34; alt=&#34;Figure 5&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are two main themes that occur in these top 20 words: fake users and software errors. When looking at these two terms (‘fake’ and ‘errors’) we see that over time, complaints of fake users (across all reviews) have nearly tripled since 2013 (&lt;em&gt;ß&lt;/em&gt; = .745, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.614, .876]) and reports of crashes have actually decreased overall (especially since 2019; &lt;em&gt;ß&lt;/em&gt; = -.270, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.458, -.080]). In fact, notice that January 2019 is when these two lines seriously diverge—the same time that ratings begin their second freefall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;6.fake_error_time.png&#34; alt=&#34;Figure 6&#34;&gt;&lt;/p&gt;
&lt;p&gt;But of course, these are not the only two relevant terms. I next used nearest neighbours word embeddings to gather the top 50 words that are the most semantically similar to ‘fake’ and ‘error’ across the reviews. Here we see words such as ‘spam,’ ‘scambots,’ and ‘phony’ co-occurring frequently with ‘fake;’ we also see ‘failed,’ ‘erorr,’ and ‘oops’ co-occurring frequently with error.  Using this collection of words, I can get a much more robust picture of how these themes were appearing over time in the reviews.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;7.fake_error_sim.png&#34; alt=&#34;Figure 7&#34;&gt;&lt;/p&gt;
&lt;p&gt;With this more robust approach we see a similar trend. Over the years the app has gotten more stable (&lt;em&gt;ß&lt;/em&gt; = -.475, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.648, -.302]), but it has also become inundated with complaints of fake profiles (&lt;em&gt;ß&lt;/em&gt; = .766, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.640, .892]). It is important to note here that the frequency of bugs over time is not a linear trend. The number of complaints increases until early 2017, and then begins to decrease. It would be appropriate to analyze this trend wholly with a higher-order (e.g., quadratic, cubic) model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;8.bugs_spam_time.png&#34; alt=&#34;Figure 8&#34;&gt;&lt;/p&gt;
&lt;p&gt;For brevity, I would like to zoom in here and focus on the important phases: the two declines, as defined by the previous breakpoints. Recall that during the initial ratings decline complaints of bugs and spam rose in tandem while during the second ratings decline only complaints of spam rose, while complaints of bugs declined. Based on this I tested two hypotheses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;During the first decline, both complaints of bugs and spam will predict lower ratings&lt;/li&gt;
&lt;li&gt;During the second decline, only complaints of spam, not bugs, will predict lower ratings.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To test these hypotheses, I ran a set of two multilevel models—one in each period of decline. Here, a multilevel model accounting for time allows us to model how reports of bugs and spam predict ratings over time. Supporting the first prediction, I found that both reports of bugs and spam predict lower ratings during the first decline. Supporting the second prediction, only reports of spam meaningfully predict lower ratings. It is also worth noting that during the periods of more stagnant ratings, complaints of neither bugs nor spam meaningfully predict decreases in ratings (Betas less than .03).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;9.betas_decline.png&#34; alt=&#34;Figure 9&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;There are two main limitations to address in these analyses. First, the gender of the reviewer is likely related to the content and ratings—especially surrounding fake users. I utilized the ‘gender’ r package to predict user’s gender based off their first names and an estimated birth year of 1990, finding that 77% of the sample (for which a first name could be determined, n = 313,901) was male. It is likely that the identified themes (e.g., fake users) are a complaint more prevalent among female users. Female users likely experience different problems on Tinder (e.g., harassment). Further probing should explore how these trends differ by gender. Secondly, me analysis of positive and negative content, and subsequently the themes of spam and bugs are only one piece impacting ratings. It is quite likely that the review data contain a great deal more nuanced information around users’ experiences on the app (e.g., frustration, harassment, disliked features, etc). Further work should again probe further into the rich natural language left by the users. Finally, I analyzed only the linear trends of the two ratings declines. A more robust version of this analysis would fit a higher order model to understand not only how ratings decline over time, but why the rise as well. It is important to understand what Tinder is doing well, particularly during the recovery phase, to get a more full picture of why ratings fluctuate (as opposed to simply decline) over time.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;In sum, we see that Tinder’s ratings have declined quite substantially between 2013 and 2022. This decline happened in stages with an initial decline, small recovery, secondary decline, and then finally stabilizing. In analyzing the natural language in nearly 600,000 reviews (nearly 7.8 million words), I find that the first decline is predicted by growing complaints of a buggy app and fake users. The second decline, however, is predicted only by the further increase of fake users. This suggests that while Tinder sufficiently stabilized the app’s performance over the years, they still need to address the growing population of fake users in order to reverse this decline in app ratings and, consequently, engagement and user retention.&lt;/p&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;changes-in-tinder-ratings-over-time&#34;&gt;Changes in Tinder Ratings Over Time&lt;/h2&gt;
&lt;h3 id=&#34;change-over-time&#34;&gt;Change Over Time.&lt;/h3&gt;
&lt;p&gt;In order to assess whether Tinder ratings were declining over time I ran a standardized linear regression predicting tinder ratings as a function of time on a daily interval. There was no missing data in the original data file. I visually assessed the linearity of the data (residuals vs. fitted plot), homogeneity of variance (scale-location plot), normality of residuals (Q-Q plot), and influential cases (residuals vs. leverage plot). There were no influential cases, and all assumptions aside from the normality of residuals were met. In order to address this assumption violation, I re-ran the model with robust standard errors (See code file). Neither the conclusions, nor the coefficients, changed. Therefore, this analysis was robust to violating the assumption of normality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;x.1.Assumptions_Ratings_Time.png&#34; alt=&#34;Figure 10&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;determining-the-trend-breakpoints&#34;&gt;Determining the Trend Breakpoints.&lt;/h3&gt;
&lt;p&gt;I utilized a piecewise linear regression to identify the breakpoints delineating in these trends. I found that tinder ratings declined from July 16, 2013 until September 24, 2015 (n = 87,588 reviews; &lt;em&gt;ß&lt;/em&gt; = -.172, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.178, -.165]). Secondly, ratings actually increased between September 25, 2015 until October 13, 2018 (n = 192,789 reviews; &lt;em&gt;ß&lt;/em&gt; = .110, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.105, .114]). Third, we observe the second ratings decline between October 14, 2018 until January 13, 2021 (n = 177,910 reviews; &lt;em&gt;ß&lt;/em&gt; = -.166, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.170, -.161]). Finally, ratings remained relatively flat from January 14, 2021 until February 18, 2022—the end of the data (n = 68,328 reviews; &lt;em&gt;ß&lt;/em&gt; = -.012, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.020, -.005]).&lt;/p&gt;
&lt;h2 id=&#34;why-are-tinder-ratings-changing-over-time&#34;&gt;Why are Tinder Ratings Changing Over Time?&lt;/h2&gt;
&lt;h3 id=&#34;text-pre-processing&#34;&gt;Text Pre-Processing.&lt;/h3&gt;
&lt;p&gt;As with any analysis of natural language, noisy and uninformative elements need to be removed from the corpus and the text must be cleaned and standardized. In keeping with natural language processing standard practices I cleaned the three in two ways. First, I converted all the text to lowercase. Second, I removed all stopwords from the text using the stopwords function contained in the r package ‘tm.’ Finally, I removed all non-English text. Given that these reviews were global, and left on mobile devices, some reviews contained non-English characters (e.g., East and South Asian alphabets) as well as non-text characters (e.g., Emojis). I removed all non-English non-text characters using the replace_non_ascii function contained in the r package ‘textclean.’ The initial corpus contained 7,830,989 words spread across 526,615 reviews. Following cleaning, I was left with 4,383,647 words spread across 520,929 reviews.&lt;/p&gt;
&lt;h3 id=&#34;computing-emotion-content&#34;&gt;Computing Emotion Content.&lt;/h3&gt;
&lt;p&gt;I conducted a two-stage process to compute the proportions of positive and negative words contained in the cleaned review text. First, I used the polite function contained in the ‘politeness’ package in r to compute positive and negative word frequencies. This package first tokenizes the text into words and then simply counts the number of positive (e.g., great, good, fun) and negative (e.g., bad, boring, terrible) words within each review. Secondly, to account for the increase in overall review content over time I computed the proportion of positive and negative words within each review (i.e., number of positive words / total word count). Thus, controlling for the number of total words, there was more positive content (&lt;em&gt;M&lt;/em&gt; = .24, &lt;em&gt;SD&lt;/em&gt; = .34) than negative content (&lt;em&gt;M&lt;/em&gt; = .08, &lt;em&gt;SD&lt;/em&gt; = .17) in the reviews. However, as seen in the main text, over time the increase in negative content outpaced the increase in positive content over time.&lt;/p&gt;
&lt;h3 id=&#34;positive-and-negative-content-over-time&#34;&gt;Positive and Negative Content Over Time.&lt;/h3&gt;
&lt;p&gt;In order to assess whether positive and negative review content were changing over time I ran two standardized linear regression predicting the proportion of positive and negative emotion content as a function of time on a daily interval. There was no missing data in the original data file. For each model, I visually assessed the linearity of the data (residuals vs. fitted plot), homogeneity of variance (scale-location plot), normality of residuals (Q-Q plot), and influential cases (residuals vs. leverage plot). In both models, there were no influential cases, and all assumptions aside from the normality of residuals were met (
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S1.Assumptions_Pos_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S1&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S2.Assumptions_Neg_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S2&lt;/a&gt;). In order to address this assumption violation, I re-ran the model with robust standard errors (See code file). Neither the conclusions, nor the coefficients, changed. Therefore, this analysis was robust to violating the assumption of normality.&lt;/p&gt;
&lt;h3 id=&#34;computing-the-most-frequently-used-negative-words&#34;&gt;Computing the Most Frequently Used Negative Words.&lt;/h3&gt;
&lt;p&gt;To compute word frequencies, I first tokenized the text of all reviews into individual words (e.g., “I love this app” becomes “I” “love” “this” “app”). Following this I conducted a sentiment analysis on this list of words using the “bing” lexicon. The bing lexicon is a dictionary of words classified in a binary fashion as either positive or negative. Specifically, I classified each word as positive or negative according to how that word is classified in the bing lexicon. I then subsetted the data to contain only negative words (total n = 308,729) and again computed the proportion of each negative word (i.e., total occurrences of each negative word / sum of all negative words) to get the relative frequency of each negative word.&lt;/p&gt;
&lt;h3 id=&#34;usage-of-fake-and-error-over-time&#34;&gt;Usage of ‘Fake’ and ‘Error’ Over Time.&lt;/h3&gt;
&lt;p&gt;In order to assess whether the usage of ‘fake’ and ‘error’ were changing over time I ran two standardized linear regression predicting the proportion of these words specifically as a function of time on a daily interval. There was no missing data in the original data file. For each model, I visually assessed the linearity of the data (residuals vs. fitted plot), homogeneity of variance (scale-location plot), normality of residuals (Q-Q plot), and influential cases (residuals vs. leverage plot). As may be intuited from the data, the trends of both ‘fake’ and ‘error’ were not linear over time nor did they conform to the assumption of normality (
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S3.Assumptions_Fake_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S3&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S4.Assumptions_Error_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S4&lt;/a&gt;). Given that this analysis was not the focal analysis, and I am not trying to make claims about these trends predicting ratings decline over the full nine-year period, I will more stringently address the violation of the linearity assumption in the final analysis. Secondarily, to address the violation of normality, I re-ran the model with robust standard errors (See code file). The conclusions do not change, nor do the coefficients change meaningfully. Therefore, this analysis was robust to violating the assumption of normality.&lt;/p&gt;
&lt;h3 id=&#34;using-word-embeddings-to-create-themes&#34;&gt;Using Word Embeddings to Create Themes.&lt;/h3&gt;
&lt;p&gt;I next utilized the word2vec package in r to detect words in the reviews that were conceptually related to the target words ‘fake’ and ‘error.’ Specifically, I used a continuous-bag-of-words (CBOW) to compute the cosine similarity of all negative words with each of the target words. I chose this algorithm as it is faster than the alternative approach, skip-grams, which are also better for infrequent words. I trained a word2vec model on the cleaned review text with 15 dimensions over 20 iterations. Following training this model, I used it to predict the top 50 words that were most conceptually similar (i.e., frequently co-occur, had the highest cosine similarity) with the target words in the semantic space. The similarity scores were overall very high to both ‘fake’ (Cosine similarity range .882 to .950) and ‘error’ (Cosine similarity range .846 to .978). Finally, I computed the frequencies of all words conceptually similar to ‘fake’ (i.e., Spam words) and ‘error’ (i.e., Bug words).&lt;/p&gt;
&lt;h3 id=&#34;usage-of-spam-and-bug-words-over-time&#34;&gt;Usage of Spam and Bug Words Over Time.&lt;/h3&gt;
&lt;p&gt;In order to assess whether the usage of Spam and Bug words were changing over time I ran two standardized linear regression predicting the proportion of these categories as a function of time on a daily interval. There was no missing data in the original data file. For each model, I visually assessed the linearity of the data (residuals vs. fitted plot), homogeneity of variance (scale-location plot), normality of residuals (Q-Q plot), and influential cases (residuals vs. leverage plot). As may be intuited from the data, the trends of both ‘fake’ and ‘error’ were not linear over time nor did they conform to the assumption of normality (
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S5.Assumptions_Bugs_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S5&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/dwiwad/Tinder_Trends/blob/main/supplemental-images/Fig_S6.Assumptions_Spam_Time.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fig. S6&lt;/a&gt;). Additionally, there appeared to be heterogeneity of variance in bug words over time. Again, this visually tracks the trend in bugs over time as the app became much more stable in the later years. To address the violations of normality and homoscedasticity, I re-ran the model with robust standard errors (See code file). The conclusions do not change, nor do the coefficients change meaningfully. Therefore, this analysis was robust to violating the assumptions of normality and homoscedasticity.&lt;/p&gt;
&lt;h3 id=&#34;bugs-and-spam-predicting-ratings&#34;&gt;Bugs and Spam Predicting Ratings.&lt;/h3&gt;
&lt;p&gt;In order to assess whether the usage of Spam and Bug words predicted ratings during the two declines I ran two standardized multilevel models. Specifically, I predicted ratings from the proportion of both Spam and Bug words while including a random intercept of time on a daily interval. There was no missing data in the original data file. In each model I computed Variance Inflation Factors (VIFs) to ensure that there were not issues of multicollinearity. All VIFs were less than 1.9, suggesting that multicollinearity was not a problem in these models. Secondly, I assessed for normality using q-q plots. In each model I found violations of this assumption. Third, I assessed for linearity using the residuals vs. fitted plot finding no violations of this assumption. To address the violations of normality and homoscedasticity, I re-ran the model with robust standard errors (See code file). The conclusions do not change, nor do the coefficients change meaningfully. Therefore, this analysis was robust to violating the assumptions of normality and homoscedasticity.&lt;/p&gt;
&lt;p&gt;Finally, I ran two more models during the other phases of ratings change: the recovery phase and final phase. As expected, neither bugs nor spam were particularly important for predicting ratings during these time periods. While all coefficients were statistically significant, no standardized effect sizes were larger than .03. In the recovery phase neither complaints of Bugs (&lt;em&gt;ß&lt;/em&gt; = -.023, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [-.035, -.011]) nor Spam (&lt;em&gt;ß&lt;/em&gt; = .032, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.021, .044]) meaningfully predicted ratings over time. Similarly, in the final phase, neither Bugs (&lt;em&gt;ß&lt;/em&gt; = .024, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.011, .037]) nor Spam (&lt;em&gt;ß&lt;/em&gt; = .016, &lt;em&gt;p&lt;/em&gt; &amp;lt; .001, 95% CI [.004, .029]) meaningfully predicted ratings over time. This final analysis lends more support to the conclusion that these experiences were driving the ratings declines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does High Economic Inequality, and Low Economic Mobility Threaten, the Relationship Between Income and Happiness?</title>
      <link>https://dwiwad.github.io/project/mobility_happiness/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 -0500</pubDate>
      <guid>https://dwiwad.github.io/project/mobility_happiness/</guid>
      <description>&lt;p&gt;Myriad past research demonstrates a fairly strong link between money and happiness. That is, people are generally happier when they make more money up to around the $75,000 a year mark 
&lt;a href=&#34;http://www.pnas.org/content/pnas/107/38/16489.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(e.g., Kahneman &amp;amp; Deaton, 2010)&lt;/a&gt;. However, one possible threat to this relationship is the level of economic inequality. Really, one could imagine this relationship going in either directions. On one hand, it seems intuitive that money has more happiness purchasing-power when your immediate community is highly unequal. In this case, having a lot of money could make you more satisfied to have &amp;lsquo;made it.&amp;rsquo; On the other hand, it could be extremely uncomfortable to be on the top of the economic ladder when poverty is so readily apparent.&lt;/p&gt;
&lt;p&gt;I tested this question with a set of multilevel models using four different datasets, which can be found 
&lt;a href=&#34;https://github.com/dwiwad/Inequality-Mobility-Income-and-Happiness/tree/master/Data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;survey_data.csv&lt;/strong&gt;; This dataset contains 1,441 survey responses from two qualtrics national panels. The key individual variables here are happiness, economic quintile, age, political ideology, and location (latitude and longitude). We collected these data in our lab as part of larger projects exploring the psychological correlates of perceived economic mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ACS_14_5YR_B19083&lt;/strong&gt;; this dataset is from the United States census and contains two county identifiers (FIPS code and county name), income inequality (Gini) for each county, and the standard error for each Gini coefficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;gini.by.state&lt;/strong&gt;; this dataset is also from the United States census and contains two state identifiers (FIPS code and state name), income inequality (Gini) for each state, and the standard error for each Gini coefficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;mobility.by.county&lt;/strong&gt;; this dataset is from the Harvard Mobility Project and contains a measure of income mobility, as well as various population demographics, for each county. The measure I will be using, absolute upward mobility, quantifies the average income percentile for a child whose parents were in the 25th percentile. So, for example, if a county has an absolute upward mobility value of 40 this means that the children of parents who were in the 25th percentile of the income distribution ended up, on average, in the 40th percentile.&lt;/p&gt;
&lt;p&gt;So, some of the data was data we had collected in the past and some of the data came from established sources. The first thing I did was take a quick look at where our participants were in the United  States.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;geo_locate.png&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, we can see that the participants are pretty spread out across the United States, with a bit of a dearth in the central United States; most participants seem to be in the Eastern U.S. I used this geolocation data to assign each participant the county in which they live. I will spare you the nitty-gritty details of the multilevel modeling, but if you are so inclined you can find everything 
&lt;a href=&#34;https://github.com/dwiwad/Inequality-Mobility-Income-and-Happiness/blob/master/Markdown%20Docs/MLM_Comprehensive_Exam.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What I found was that, in line with past research, personal income (as measured by which income quintile the person reports being in) was a very strong predictor of happiness. In accounting for all the individual (age, gender, political ideology) and county-level (inequality) factors, it income was the only significant determinant of happiness. While I didn&amp;rsquo;t observe a significant interaction between income and happiness, suggesting that there could be a relationship here. But what does that relationship actually look like?&lt;/p&gt;
&lt;p&gt;In order to take a look at this, I did a quartile split on income inequality and binned participants into four categories: extremely low inequality (gini less than .43), low inequality (gini between .43 and .45), high inequality (gini more than .45 and less than.48), and very  high inequality (gini more than .48). Then, I plotted out the simple relationshp between income quintile and happiness in each of these inequality categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ineq_hap_simp.png&#34; alt=&#34;Figure 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;What you can see here pretty clearly is that the slope of the relationship in counties with less inequality (the green and orange lines) is steeper than in counties with more inequality (the purple and pink lines). That is to say, money seems to have more &amp;ldquo;happiness purchasing power&amp;rdquo; when people live in more equal communities. Just to look at this in another way, here are the data plotted as actual means instead of over simplified regression lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ineq_hap.png&#34; alt=&#34;Figure 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above presented data show that the increase we get in happiness from having more money is diminished in places with higher inequality. One possible reason for this is simply exposure to inequality through poverty. Looking only at the people who reported being in the fifth quintile, you can see that happiness is the lowest in the high and extremely high inequality places. It’s possible that in these places the high income people are exposed to more poverty, and thus feel an increased sense of wealth guilt, leading to a dampened general happiness.&lt;/p&gt;
&lt;p&gt;This is only one possible explanation and uncovering the mechanism here requires significant further exploration. For now, I’m going to just explore the data again, this time looking at the level of absolute upward mobility present in a county, instead of inequality.&lt;/p&gt;
&lt;h2 id=&#34;income-and-happiness-by-county-upward-mobility&#34;&gt;Income and Happiness by County Upward Mobility&lt;/h2&gt;
&lt;p&gt;One might suspect an interaction here such that the relationship between income and happiness is stronger in places with low mobility, perhaps as a sort of dissonance mechanism. That is, when one cannot move up the income ladder they rationalize and are thus happier with their level of income, regardless. Specifically, I would think that high income people are equally happy regardless of the level of mobility, but as mobility drops the baseline level of happiness for those in lower quintiles rises. Same with the previous analysis, I&amp;rsquo;m going to spare you all the nitty-gritty modeling details here and just present the main trends.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mob_hap_simp.png&#34; alt=&#34;Figure 4&#34;&gt;&lt;/p&gt;
&lt;p&gt;This actually doesnt look all that different from the inequality version of the graph, but here there is no interaction whatsoever. The slopes are all equal. Let’s take a quick look at the graph of the actual data, instead of the regression lines:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mob_hap.png&#34; alt=&#34;Figure 5&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking at this graph versus the graph with county inequality makes it clear there really is no interaction here, just as the MLM data show. These models suggest that the relationship between income and happiness may indeed be influenced by the level of inequality present in the county one lives. Specifically, income has greater happiness purchasing power when you live in a more equal society. Perhaps this is due to the decreased availability of visible poverty and inequality, leading to a lower sense of wealth guilt among those living in the higher quintiles. On the other hand, it is also possible that in areas with lower inequality do not suffer so much from the middle class being washed out, thus having a higher income means one’s purchasing power is higher and can live relatively better off compared with someone of the same income bracket in a high-inequality city, where their money potentiall has less purchasing power.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In sum, across this analysis I: (a) cleaned and combined four separate datasets into one useable dataset with individual and county level information, (b) ran a series of multilevel models exploring the interaction of county-level data and individual level relationships, and (C) unpacked these interactions with concise data visualization. Within the analyses, I first replicated a long-standing effect showing that higher wealth is related to higher happiness, overall. I then built upon this, showing that there appears to be a modest interaction between the level of inequality where one lives and the strength of the money-happiness relationship. Particularly, it appears that the happiness-purchasing power of money is greater when one lives in a more equal county. Lastly, I found that the level of absolute upward mobility in a county does not change the nature of the money-happiness relationship&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
